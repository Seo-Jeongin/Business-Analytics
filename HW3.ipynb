{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3b6e66",
   "metadata": {},
   "source": [
    "# 텍스트 마이닝의 이론과 실제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd977f7",
   "metadata": {},
   "source": [
    "## 1. 텍스트 마이닝의 이해\n",
    "### 텍스트 마이닝(Text Mining)이란?\n",
    "* 위키피디아 : 텍스트로부터 양질의 정보를 뽑아내는 과정\n",
    "* 양질의 정보는 통계적인 패턴 학습을 통해 텍스트 안에 숨어있는 패턴이나 경향으로부터 추출\n",
    "* 목표는 비정형 데이터를 분석이 가능한 정형 데이터로 변형하는 것\n",
    "* 일정한 길이의 vector로 변환 -> 변환된 vector에 머신러닝(딥러닝) 기법을 적용\n",
    "\n",
    "### 텍스트 마이닝의 이해를 위한 기본요구지식\n",
    "* 자연어 처리\n",
    "* 통계학 & 선형대수\n",
    "* 머신러닝\n",
    "* 딥러닝\n",
    "\n",
    "### 텍스트 마이닝 방법\n",
    "* NLP(Natural Language Processing) 기본도구 : Tokenize, stemming, lemmatize, Chunking, BOW, TFIDF\n",
    "* 머신러닝(딥러닝) : Logistic egression, Embedding\n",
    "\n",
    "### 텍스트 마이닝 단계\n",
    "* Documnet -> \"tokenize, normalize\" -> Sequence of normalized words\n",
    "  + Fixed size vector without sequence info. ex) BOW, TFIDF\n",
    "  + Fixed size vector with sequence info. ex) Doc2Vec\n",
    "  + Series of Word Embedding with sequence info.\n",
    " \n",
    "### 텍스트 마이닝 적용분야\n",
    "* Document classification\n",
    "* Document generation\n",
    "* Keyword extraction\n",
    "* Topic odelinga\n",
    "\n",
    "### 텍스트 마이닝 도구 - 파이썬\n",
    "* NLTK : NLP 라이브러리\n",
    "* Scikit Learn : 머신러닝 라이브러리\n",
    "* Gensim : Word2Vec으로 유명\n",
    "* Keras : 딥러닝 위주의 라이브러리 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85af3229",
   "metadata": {},
   "source": [
    "## 2. 텍스트 마이닝 방법론\n",
    "* 목적 : documnet, sentence 등을 sparse vector로 변환\n",
    "\n",
    "### 1) Tokenize\n",
    "* Documnet를 Sentence의 집합으로 분리\n",
    "* Sentence를 Word의 집합으로 분리\n",
    "\n",
    "### 2) Text Normalization\n",
    "* 최소 단위를 표준화\n",
    "* Stemming (어간 추출) : 규칙에 의한 변환\n",
    "* Lemmatization (표제어 추출) : 사전을 이용하여 단어의 원형 추출\n",
    "\n",
    "### 3) POS-tagging\n",
    "* 최소 의미단위로 나누어진 대상에 대해 품사 부착\n",
    "* 각 단어에 대해 올바른 발음을 하기 위해 품사 태깅 이용\n",
    "\n",
    "### 4) Chunking\n",
    "* POS-tagging의 결과를 말모듬으로 다시 합치는 과정\n",
    "* 형태소들을 서로 겹치지 않으면서 의미가 있는 구로 묶어나가는 과정\n",
    "* 텍스트로부터 Information Extraction(정보추출)을 하기 위한 전단계 혹은 포함\n",
    "  + 개체명 인식(Named Entitiy Recognition, NER) : 텍스트로부터 의미 있는 정보를 추출하기 위한 방법으로 사용\n",
    "\n",
    "### 5) BOW (Bag of Words)\n",
    "* Vecotr Space Model : 모든 문서에서 한 번 이상 나타난 단어들에 대해 유(1)/무(0)로 표현\n",
    "* Count vector : 단어가 문서에 나타난 횟수로 표현\n",
    "\n",
    "### 6) TFIDF (Term Frequency - Inverse Document Frequency)\n",
    "* 단어의 count를 단어가 나타난 문서의 수로 나눠서 자주 등장하지 않는 단어의 weight를 올림\n",
    "* TFIDF를 이용한 유사도 계산\n",
    "  + TFIDF Matching score : TFIDF vector의 내적 이용\n",
    "  + Cosine Similarity : vector의 방향에 대한 유사도\n",
    "\n",
    "### 7) Text Classification with BOW/TFIDF\n",
    "* Naive Bayes : text categorization에 많이 사용\n",
    "* Ridge and Lasso Regression\n",
    "  + 릿지 회귀(Ridge regression)\n",
    "  + 라쏘 회귀(Lasso regression)\n",
    "\n",
    "### 8) 문서분류의 활용 - Sentiment Analysis\n",
    "* 소비자의 감성과 관련된 텍스트 정보를 자동으로 추출하는 텍스트 마이닝 기술의 한 영역\n",
    "* 한글 감성분석\n",
    "  + 예: label이 0이면 부정, 1이면 긍정\n",
    "  + 리뷰를 BOW로 변환 후 input으로, label을 target으로하여 학습\n",
    "  + 새로운 리뷰에 대해 긍정/부정 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c95f0",
   "metadata": {},
   "source": [
    "## 3. 텍스트 마이닝의 문제\n",
    "### 1) Curse of Dimensionality \n",
    "* 차원의 저주 : 각 데이터 간의 거리가 너무 멀게 위치\n",
    "* 해결방안 : 더 많은 데이터, 차원 축소\n",
    "\n",
    "### 2) 단어 빈도의 불균형\n",
    "* Zipf's law(멱법칙) : 극히 소수의 데이터가 결정적인 영향을 미침\n",
    "* 해결방안\n",
    "  + feature selection : 빈도 높은 단어 삭제\n",
    "  + Boolean BOW 사용 : 유/무만 사용 \n",
    "\n",
    "### 3) 단어가 쓰인 순서정보의 손실\n",
    "* 단어들의 순서 - context가 중요\n",
    "* 해결방안\n",
    "  + n-gram : classification 문제에서 유용\n",
    "  + Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9655b00",
   "metadata": {},
   "source": [
    "## 4. 문제의 해결 방안\n",
    "### Dimentionality Reduction (차원 축소)\n",
    "* Feature selection\n",
    "* Feature extraction\n",
    "  + PCA(Principal Component Analysis, 주성분 분석) : 데이터의 분산을 초대한 보존하는 새로운 축을 찾아 변환함으로써 차원을 축소\n",
    "  + LSA(Latent Semantic Analysis, 잠재 의미 분석) : 문서간의 유사도, 단어간의 유사도 측정 활용\n",
    "    + SVD(Singular Vector Decomposition, 특이값 분해)\n",
    "* Embedding\n",
    "* Deep Learning\n",
    "\n",
    "#### RBM (Restricted Boltzmann Machine)\n",
    "* 차원을 변경하면서 원래의 정보량 최대한 유지하는 것이 목적\n",
    "* 사전학습을 통한 차원 축소에서 사용 가능\n",
    "\n",
    "#### Autoencoder\n",
    "* RBM과 유사한 개념 : encoder로 차원을 축소하고 decoder로 다시 복원했을 때, 원래의 X와 복원한 X’이 최대한 동일하도록 학습\n",
    "* 작동방식은 PCA와 유사\n",
    "\n",
    "### Topic Modeling\n",
    "* 토픽은 주제를 의미하는 용어 사용되며, 각 문서들이 특정한 주제에 속할 확률분포와 특정 단어들이 파생되어 나올 확률분포가 주어졌을 때, 이 두 확률분포를 조합하여 각 문서들에 들어가는 단어들의 확률분포를 계산\n",
    "\n",
    "### Word Embedding\n",
    "* 단어에 대한 vector의 dimension reduction이 목표\n",
    "* one-hot-encoding으로 표현된 단어를 dense vector로 변환\n",
    "  + one-hot-encoding : 각 단어를 모든 문서에서 사용된 단어들의 수 길이의 벡터로 표현\n",
    "* 변환된 vector를 이용하여 학습\n",
    "* 최종목적에 맞게 학습에 의해 vector 결정\n",
    "* 학습목적 관점에서의 단어의 의미를 내포\n",
    "* Word Embedding을 이용한 문서 분류\n",
    "  + Documnet : 제한된 maxlen개의 word sequence(앞이나 뒤를 잘라냄), (maxlen, reduced_dim)dml 2차원 행렬로 표현\n",
    "  + Word : one-hot-vector에서 저차원으로 embedding된 dense vector\n",
    "  + 단순한 분류모형(sequence 무시)\n",
    "\n",
    "#### Word2Vec\n",
    "* 문장에 나타난 단어들의 순서를 이용해 word embedding을 수행\n",
    "  + CBOW : 주변단어들을 이용해 다음 단어를 예측\n",
    "  + Skip-gram : 한 단어의 주변단어들을 예측\n",
    "* 의미 : 단어의 위치에 기반하여 의미를 내포하는 vector 생성\n",
    "  + 비슷한 위치에 나타나는 단어들은 비슷한 vector를 가지게 됨\n",
    "  + 단어 간의 유사성을 이용하여 연산 가능\n",
    "\n",
    "#### ELMo (Embeddings from Language Model)\n",
    "* 사전 훈련된 언어 모델을 사용하는 워드 임베딩 방법론\n",
    "* 문맥을 반영하기 위해 개발된 워드 임베딩 기법\n",
    "* 문맥의 파악을 위해 biLSTM으로 학습된 모형 이용\n",
    "\n",
    "### Transfer Learning(전이 학습)\n",
    "* feature level: 단어에 대한 dense vector(word embedding)를 새로 학습하지 않고 학습된 vector를 그대로 가져다 씀\n",
    "* model level: word embedding과 모형 전체를 가져다 학습\n",
    "\n",
    "### Document Embedding\n",
    "* Word2Vec 모형에서 주변단어들에 더하여 document의 고유한 vector를 함께 학습함으로써 document에 대한 dense vector를 생성\n",
    "* 이 dense vector를 이용해 매칭, 분류 등의 작업 수행\n",
    "\n",
    "### Context(sequence)의 파악\n",
    "* N-gram\n",
    "  + bi-gram, tri-gram 등\n",
    "  + 대상이 되는 문자열을 하나의 단어 단위가 아닌, 두개 이상의 단위로 잘라서 처리\n",
    "* 딥러닝 - RNN\n",
    "  + 문장을 단어들의 sequence 혹은 series로 처리\n",
    "  + 뒷 단어에 대한 hidden node가 앞 단어의 hidden node 값에도 영향을 받도록 함\n",
    "  \n",
    "### LSTM (Long Short Term Memory)\n",
    "* RNN의 문제 : 문장이 길수록 층이 깊은 형태를 갖게 됨 -> 경사가 소실되는 문제 발생 -> 앞부분의 단어 정보가 학습되지 않음\n",
    "* LSTM : 직통 통로를 만들어 RNN의 문제 해결\n",
    "* Bi-LSTM\n",
    "  + 양방향으로 LSTM을 구성하여 두 결과를 합침\n",
    "  + 양방향 순서를 모두 학습\n",
    "\n",
    "### 합성곱 신경망 (Convolutional Neural Networks, CNN)\n",
    "* CNN은 원래 이미지 처리를 위해 개발된 신경망으로, 현재는 인간의 이미지 인식보다 더 나은 인식 성능을 보이고 있음\n",
    "* CNN은 합성곱층(conolution layer)와 풀링층(pooling)으로 구성되며, 합성곱층은 2차원 이미지에서 특정 영역의특징을 추출하는 역할을 하는데, 이는 연속된 단어들의 특징을 추출하는 것과 유사한 특성이 있음\n",
    "* CNN을 이용한 문서 분류 : 단어 시퀀스에 대해 CNN의 필터는 1차원으로만 적용됨\n",
    "\n",
    "### Sequence-to-sequence\n",
    "* 번역, chat-bot, summarize등은 입력과 출력 모두 sequence가 되어야 함\n",
    "* encoder, decoder의 구조를 가짐\n",
    "\n",
    "#### Attention\n",
    "* 출력에 나온 어떤 단어는 입력에 있는 특정 단어들에 민감한 것에 착안\n",
    "* 입력의 단어들로부터 출력 단어에 직접 링크를 만듦\n",
    "\n",
    "#### Transfomer (Self-Attention)\n",
    "* 입력 단어들끼리도 상호연관성이 있는 것에 착안\n",
    "  + 입력 -> 출력으로의 attention 외에 입력 단어들 간의 attention, 입력+출력 -> 출력으로의 attention 추가\n",
    "* encoder와 decoder가 서로 다른 attention 구조 사용\n",
    "\n",
    "#### BERT (Bidirectional Encoder Representations form Transformer)\n",
    "* 양방향 transformer 인코더 사용\n",
    "* transfer learning에서 feature+model을 함께 transfer하고 fine tuning을 통해서 적용하는 방식 선택\n",
    "* segment, position embedding 사용\n",
    "* 다양한 text mining task에 전이학습을 이용해 적용 가능한 구조 제안"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac014213",
   "metadata": {},
   "source": [
    "# 웹 크롤링1 - Static Crwling\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b11445",
   "metadata": {},
   "source": [
    "## 1. urllib\n",
    "* 파이썬은 웹 사이트에 있는 데이터를 추출하기 위해 urllib 라이브러리 사용\n",
    "* 이를 이용해 HTTP 또는 FTP를 사용해 데이터 다운로드 가능\n",
    "* urllib은 URL을 다루는 모듈을 모아 놓은 패키지\n",
    "* urllib.request 모듈은 웹 사이트에 있는 데이터에 접근하는 기능 제공, 또한 인증, 리다렉트, 쿠키처럼 인터넷을 이용한 다양한 요청과 처리가 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d8c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c6bb2e",
   "metadata": {},
   "source": [
    "### 1.1. urllib.request를 이용한 다운로드\n",
    "* urllib.request 모듈에 있는 urlretrieve() 함수 이용\n",
    "* 다음의 코드는 PNG 파일을 test.png 라는 이름의 파일로 저장하는 예제임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3412621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 읽어들이기 \n",
    "from urllib import request\n",
    "\n",
    "# URL과 저장경로 지정하기\n",
    "url=\"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename=\"test.png\"\n",
    "\n",
    "request.urlretrieve(url, savename)  # urlretrieve 함수를 통해 test.png에 자료 입력하여 파일에 직접 저장\n",
    "print(\"저장되었습니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed9e1e",
   "metadata": {},
   "source": [
    "### 1.2. urlopen으로 파일에 저장하는 방법\n",
    "* request.urlopen()은 메모리에 데이터를 올린 후 파일에 저장하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f855356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다..\n"
     ]
    }
   ],
   "source": [
    "# URL과 저장경로 지정하기\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"test1.png\"\n",
    "#다운로드\n",
    "mem = request.urlopen(url).read()  # 읽어온 데이터를 임의의 변수 mem에 저장\n",
    "#파일로 저장하기, wb는 쓰기와 바이너리모드\n",
    "with open(savename, mode=\"wb\") as f:\n",
    "    f.write(mem)\n",
    "    print(\"저장되었습니다..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262d789",
   "metadata": {},
   "source": [
    "### 1.3. API 사용하기\n",
    "#### 클라이언트 접속 정보 출력 (기본)\n",
    "* API는 사용자의 요청에 따라 정보를 반환하는 프로그램\n",
    "* IP 주소, UserAgent 등 클라이언트 접속정보 출력하는 \"IP 확인 API\" 접근해서 정보를 추출하는 프로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f513a92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ip]\n",
      "API_URI=http://api.aoikujira.com/ip/get.php\n",
      "REMOTE_ADDR=221.141.50.136\n",
      "REMOTE_HOST=221.141.50.136\n",
      "REMOTE_PORT=33376\n",
      "HTTP_HOST=api.aoikujira.com\n",
      "HTTP_USER_AGENT=Python-urllib/3.8\n",
      "HTTP_ACCEPT_LANGUAGE=\n",
      "HTTP_ACCEPT_CHARSET=\n",
      "SERVER_PORT=80\n",
      "FORMAT=ini\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#데이터 읽어들이기\n",
    "url=\"http://api.aoikujira.com/ip/ini\"\n",
    "res=request.urlopen(url)  # 데이터를 res 변수에 저장\n",
    "data=res.read()  # 데이터 읽어옴\n",
    "\n",
    "#바이너리를 문자열로 변환하기\n",
    "text=data.decode(\"utf-8\")  # decode()를 이용해 바이너리를 문자열로 변환\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d1297e",
   "metadata": {},
   "source": [
    "## 2. BeautifulSoup\n",
    "* 스크레이핑(Scraping or Crawling)이란 웹 사이트에서 데이터를 추출하고, 원하는 정보를 추출하는 것을 의미\n",
    "* BeautifulSoup란 파이썬으로 스크레이핑할 때 사용되는 라이브러리로서 HTML/XML에서 정보를 추출할 수 있도록 도와줌. 그러나 다운로드 기능은 없음.\n",
    "* 파이썬 라이브러리는 pip 명령어를 이용해 설치 가능. Python Package Index(PyPI)에 있는 패키지 명령어를 한줄로 설치 가능\n",
    "  + URL (http://pypi.python.org/pypi)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e539987",
   "metadata": {},
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba2a803",
   "metadata": {},
   "source": [
    "* 예제 HTML\n",
    "```python\n",
    "<html><body>\n",
    "  <h1>스크레이핑이란?</h1>\n",
    "  <p>웹 페이지를 분석하는 것</p>\n",
    "  <p>원하는 부분을 추출하는 것</p>\n",
    "</body></html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bfc58",
   "metadata": {},
   "source": [
    "#### 패키지 import 및 예제 HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e0cfb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  # 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ce3582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html 소스 입력하기\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <h1>스크레이핑이란?</h1>\n",
    "  <p>웹 페이지를 분석하는 것</p>\n",
    "  <p>원하는 부분을 추출하는 것</p>\n",
    "</body></html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaac46d9",
   "metadata": {},
   "source": [
    "### 2.1. 기본 사용\n",
    "* 다음은 Beautifulsoup를 이용하여 웹사이트로부터 HTML을 가져와 문자열로 만들어 이용하는 예제임\n",
    "* h1 태그를 접근하기 위해 html-body-h1 구조를 사용하여 soup.html.body.h1 이런식으로 이용하게 됨.\n",
    "* p 태그는 두개가 있어 soup.html.body.p 한 후 next_sibling을 두번 이용하여 다음 p를 추출. 한번만 하면 그 다음 공백이 추출됨.\n",
    "* HTML 태그가 복잡한 경우 이런 방식으로 계속 진행하기는 적합하지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb376a1",
   "metadata": {},
   "source": [
    "#### 2) HTML 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74d08557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬 내장 html.parser를 이용하여 html을 파이썬 객체로 변환\n",
    "soup = BeautifulSoup(html, 'html.parser')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82a64bd",
   "metadata": {},
   "source": [
    "#### 3) 원하는 부분 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cf15a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = soup.html.body.h1  # html -> body -> h1 태그 출력\n",
    "p1 = soup.html.body.p  # html -> body -> p 태그 출력\n",
    "p2 = p1.next_sibling.next_sibling  # p1 다음 문자열 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55697243",
   "metadata": {},
   "source": [
    "#### 4) 요소의 글자 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64a1204b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 스크레이핑이란?\n",
      "p  = 웹 페이지를 분석하는 것\n",
      "p  = 원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "# 추출한 것을 문자열로 출력\n",
    "print(f\"h1 = {h1.string}\")\n",
    "print(f\"p  = {p1.string}\")\n",
    "print(f\"p  = {p2.string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8026d372",
   "metadata": {},
   "source": [
    "### 2.2. 요소를 찾는 method\n",
    "#### 단일 element 추출: find()\n",
    "BeautifulSoup는 루트부터 하나하나 요소를 찾는 방법 말고도 find()라는 메소드를 제공함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "027b7bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf2a5e0",
   "metadata": {},
   "source": [
    "* 1) find() 메서드로 원하는 부분 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcd17987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>스크레이핑이란?</h1>\n"
     ]
    }
   ],
   "source": [
    "title = soup.find(\"h1\")  # h1 태그를 찾아 추출\n",
    "body  = soup.find(\"p\")  # p 태그를 찾아 추출\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e469abd6",
   "metadata": {},
   "source": [
    "* 2) 텍스트 부분 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7525fab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#title = 스크레이핑이란?\n",
      "#body = 웹 페이지를 분석하는 것\n"
     ]
    }
   ],
   "source": [
    "# 문자열로 출력\n",
    "print(f\"#title = {title.string}\" )\n",
    "print(f\"#body = {body.string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c69c5cb",
   "metadata": {},
   "source": [
    "#### 복수 elements 추출: find_all()\n",
    "여러개의 태그를 한번에 추출하고자 할때 사용함. 다음의 예제에서는 여러개의 태그를 추출하는 법을 보여주고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8cdbccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html 소스 입력하기\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <ul>\n",
    "    <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "    <li><a href=\"http://www.daum.net\">daum</a></li>\n",
    "  </ul>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "# 파이썬 내장 html.parser를 이용하여 html을 파이썬 객체로 변환\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842de4ca",
   "metadata": {},
   "source": [
    "* 1) find_all() 메서드로 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f989995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"http://www.naver.com\">naver</a>, <a href=\"http://www.daum.net\">daum</a>] 2\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all(\"a\")  # a 태그 문자열 모두 추출\n",
    "print(links, len(links))  # 찾은 문자열과 개수 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93db3a4",
   "metadata": {},
   "source": [
    "* 2) 링크 목록 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb8cecec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver > http://www.naver.com\n",
      "daum > http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "for a in links:\n",
    "    href = a.attrs['href'] # a 태그의 href의 속성에 있는 속성값을 추출\n",
    "    text = a.string # a 태그의 문장 추출\n",
    "    print(text, \">\", href)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e5d49b",
   "metadata": {},
   "source": [
    "## 3. Css Selector\n",
    "\n",
    "Css Selector란, 웹상의 요소에 css를 적용하기 위한 문법으로, 즉 요소를 선택하기 위한 패턴입니다.\n",
    "\n",
    "서식         |설명\n",
    "-------------|---------------\n",
    "*            |모든 요소를 선택\n",
    "<요소 이름>  |요소 이름을 기반으로 선택\n",
    "<클래스 이름>|클래스 이름을 기반으로 선택\n",
    "#<id 이름>   |id 속성을 기반으로 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ccf184",
   "metadata": {},
   "source": [
    "### BeautifulSoup에서 Css Selector 사용하기\n",
    "BeautifulSoup에서는 Css Selector로 값을 가져올 수 있도록 find와는 다른 다음과 같은 메서드를 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82548b22",
   "metadata": {},
   "source": [
    "메서드                 |설명\n",
    "-----------------------|-----\n",
    "soup.select_one(선택자)|CSS 선택자로 요소 하나를 추출합니다.\n",
    "soup.select(선택자)    |CSS 선택자로 요소 여러 개를 리스트를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21e3515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html><body>\n",
    "<div id=\"meigen\">\n",
    "  <h1>위키북스 도서</h1>\n",
    "  <ul class=\"items\">\n",
    "    <li>유니티 게임 이펙트 입문</li>\n",
    "    <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "    <li>모던 웹사이트 디자인의 정석</li>\n",
    "  </ul>\n",
    "</div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석하기 \n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c4342",
   "metadata": {},
   "source": [
    "* 필요한 부분을 CSS 쿼리로 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9383a1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 위키북스 도서\n",
      "li = 유니티 게임 이펙트 입문\n",
      "li = 스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "li = 모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "# 타이틀 부분 추출하기 --- (※3)\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string\n",
    "print(f\"h1 = {h1}\")\n",
    "\n",
    "# 목록 부분 추출하기 --- (※4)\n",
    "li_list = soup.select(\"div#meigen > ul.items > li\")\n",
    "for li in li_list:\n",
    "  print(f\"li = {li.string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9363250",
   "metadata": {},
   "source": [
    "## 4. 활용 예제\n",
    "앞서 배운 urllib과 BeautifulSoup를 조합하면, 웹스크레이핑 및 API 요청 작업을 쉽게 수행하실 수 있습니다.\n",
    "\n",
    "1. URL을 이용하여 웹으로부터 html을 읽어들임 (urllib)\n",
    "2. html 분석 및 원하는 데이터를 추출 (BeautifulSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0f67b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request, parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2db91e",
   "metadata": {},
   "source": [
    "### 4.1. 네이버 금융 - 환율 정보\n",
    "* 다양한 금융 정보가 공개돼 있는 \"네이버 금융\"에서 원/달러 환율 정보를 추출해보자!\n",
    "* 네이버 금융의 시장 지표 페이지 https://finance.naver.com/marketindex/\n",
    "* 다음은 원/달러 환율 정보를 추출하는 프로그램임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a1afa",
   "metadata": {},
   "source": [
    "#### 1) HTML 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5878e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = request.urlopen(url)  # 파일에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769ca487",
   "metadata": {},
   "source": [
    "#### 2) HTML 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e90992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4f145",
   "metadata": {},
   "source": [
    "#### 3) 원하는 데이터 추출하기¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c432fc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd/krw = 1,178.00\n"
     ]
    }
   ],
   "source": [
    "price = soup.select_one(\"div.head_info > span.value\").string\n",
    "print(\"usd/krw =\", price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56813794",
   "metadata": {},
   "source": [
    "### 4.2. 기상청 RSS\n",
    "* 기상청 RSS에서 특정 내용을 추출하는 예제\n",
    "* 기상청 RSS에서 XML 데이터를 추출하고 XML 내용을 출력\n",
    "* 기상청의 RSS 서비스에 지역 번호를 지정하여 데이터 요청해보기 http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\n",
    "  + 참고: 기상청 RSS http://www.kma.go.kr/weather/lifenindustry/service_rss.jsp\n",
    "  \n",
    "매개변수    |의미\n",
    "------------|-----\n",
    "stnid       |기상정보를 알고 싶은 지역을 지정\n",
    "\n",
    "* 지역 번호는 다음과 같음\n",
    "\n",
    "지역       |지역번호|지역          |지역번호\n",
    "-----------|--------|--------------|--------\n",
    "전국       |108     |전라북도      |146\n",
    "서울/경기도|109     |전라남도      |156\n",
    "강원도     |105     |경상북도      |143\n",
    "충청북도   |131     |경상남도      |159\n",
    "충청남도   |133     |제주특별자치도|184\n",
    "\n",
    "* 파이썬으로 요청 전용 매개변수를 만들 때는 urllib.parse 모듈의 urlencode() 함수를 사용해 매개변수를 URL로 인코딩한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f311f078",
   "metadata": {},
   "source": [
    "#### 1) HTML 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e002fc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url= http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "\n",
    "#매개변수를 URL로 인코딩한다.\n",
    "values = {\n",
    "    'stnId':'109'\n",
    "}\n",
    "\n",
    "params=parse.urlencode(values)\n",
    "url += \"?\"+params # URL에 매개변수 추가\n",
    "print(\"url=\", url)\n",
    "\n",
    "res = request.urlopen(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adbb6bf",
   "metadata": {},
   "source": [
    "#### 2) HTML 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "125135e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8f9cc",
   "metadata": {},
   "source": [
    "#### 3) 원하는 데이터 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78a58597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울,경기도 육상중기예보\n",
      "○ (강수) 29일(수)은 비가 내리겠습니다.<br />○ (기온) 이번 예보기간 아침최저기온은 13~19도로 오늘(25일, 아침최저기온 15~20도)과 비슷하거나 조금 낮겠고, <br />          낮최고기온은 23~28도로 오늘(25일, 낮최고기온 23~24도)보다 높겠습니다.<br />○ (해상) 서해중부해상의 물결은 1.0~2.0m로 일겠습니다.\n"
     ]
    }
   ],
   "source": [
    "header = soup.find(\"header\")  # header 태그 찾아 추출\n",
    "\n",
    "title = header.find(\"title\").text  # header의 title 텍스트로 추출\n",
    "wf = header.find(\"wf\").text  # header의 wf 테스트로 추출\n",
    "\n",
    "print(title)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5caf85",
   "metadata": {},
   "source": [
    "* css selector 기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c5478b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울,경기도 육상중기예보\n",
      "○ (강수) 29일(수)은 비가 내리겠습니다.<br />○ (기온) 이번 예보기간 아침최저기온은 13~19도로 오늘(25일, 아침최저기온 15~20도)과 비슷하거나 조금 낮겠고, <br />          낮최고기온은 23~28도로 오늘(25일, 낮최고기온 23~24도)보다 높겠습니다.<br />○ (해상) 서해중부해상의 물결은 1.0~2.0m로 일겠습니다.\n"
     ]
    }
   ],
   "source": [
    "title = soup.select_one(\"header > title\").text  # 타이틀 부분 추출\n",
    "wf = header.select_one(\"header wf\").text  # wf 부분 추출\n",
    "\n",
    "print(title)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d80455",
   "metadata": {},
   "source": [
    "### 4.3. 윤동주 작가의 작품 목록\n",
    "* 위키문헌 (https://ko.wikisource.org/wiki) 에 공개되어 있는 윤동주의 작품목록을 가져오기\n",
    "* 윤동주 위키 (https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC)\n",
    "* 하늘과 바람과 시 부분을 선택한 후 오른쪽 마우스 이용해 copy selector로 카피하면 다음의 CSS 선택자가 카피됨\n",
    "  + #mw-content-text > div > ul:nth-child(6) > li > b > a\n",
    "* nth-child(n) 은 n 번째 요소를 의미 즉 6번째 요소를 의미, #mw-content-text 내부에 있는 url 태그는 모두 작품과 관련된 태그. 따라서 따로 구분할 필요는 없으며 생략해도 됨. BeautifulSoup는 nth-child 지원하지 않음\n",
    "  + Recall PR7 Problem1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ceaf4e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 하늘과 바람과 별과 시\n",
      "- 증보판\n",
      "- 서시\n",
      "- 자화상\n",
      "- 소년\n",
      "- 눈 오는 지도\n",
      "- 돌아와 보는 밤\n",
      "- 병원\n",
      "- 새로운 길\n",
      "- 간판 없는 거리\n",
      "- 태초의 아침\n",
      "- 또 태초의 아침\n",
      "- 새벽이 올 때까지\n",
      "- 무서운 시간\n",
      "- 십자가\n",
      "- 바람이 불어\n",
      "- 슬픈 족속\n",
      "- 눈감고 간다\n",
      "- 또 다른 고향\n",
      "- 길\n",
      "- 별 헤는 밤\n",
      "- 흰 그림자\n",
      "- 사랑스런 추억\n",
      "- 흐르는 거리\n",
      "- 쉽게 씌어진 시\n",
      "- 봄\n",
      "- 참회록\n",
      "- 간(肝)\n",
      "- 위로\n",
      "- 팔복\n",
      "- 못자는밤\n",
      "- 달같이\n",
      "- 고추밭\n",
      "- 아우의 인상화\n",
      "- 사랑의 전당\n",
      "- 이적\n",
      "- 비오는 밤\n",
      "- 산골물\n",
      "- 유언\n",
      "- 창\n",
      "- 바다\n",
      "- 비로봉\n",
      "- 산협의 오후\n",
      "- 명상\n",
      "- 소낙비\n",
      "- 한난계\n",
      "- 풍경\n",
      "- 달밤\n",
      "- 장\n",
      "- 밤\n",
      "- 황혼이 바다가 되어\n",
      "- 아침\n",
      "- 빨래\n",
      "- 꿈은 깨어지고\n",
      "- 산림\n",
      "- 이런날\n",
      "- 산상\n",
      "- 양지쪽\n",
      "- 닭\n",
      "- 가슴 1\n",
      "- 가슴 2\n",
      "- 비둘기\n",
      "- 황혼\n",
      "- 남쪽 하늘\n",
      "- 창공\n",
      "- 거리에서\n",
      "- 삶과 죽음\n",
      "- 초한대\n",
      "- 산울림\n",
      "- 해바라기 얼굴\n",
      "- 귀뚜라미와 나와\n",
      "- 애기의 새벽\n",
      "- 햇빛·바람\n",
      "- 반디불\n",
      "- 둘 다\n",
      "- 거짓부리\n",
      "- 눈\n",
      "- 참새\n",
      "- 버선본\n",
      "- 편지\n",
      "- 봄\n",
      "- 무얼 먹구 사나\n",
      "- 굴뚝\n",
      "- 햇비\n",
      "- 빗자루\n",
      "- 기왓장 내외\n",
      "- 오줌싸개 지도\n",
      "- 병아리\n",
      "- 조개껍질\n",
      "- 겨울\n",
      "- 트루게네프의 언덕\n",
      "- 달을 쏘다\n",
      "- 별똥 떨어진 데\n",
      "- 화원에 꽃이 핀다\n",
      "- 종시\n"
     ]
    }
   ],
   "source": [
    "# html 가져오기\n",
    "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\"\n",
    "res = request.urlopen(url)\n",
    "# html 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# #mw-content-text 바로 아래에 있는 \n",
    "# ul 태그 바로 아래에 있는\n",
    "# li 태그 아래에 있는\n",
    "# a 태그를 모두 선택합니다.\n",
    "a_list = soup.select(\"#mw-content-text   ul > li  a\")  # css.selector 이용하여 추출\n",
    "for a in a_list:\n",
    "    name = a.string\n",
    "    print(f\"- {name}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f3ae1d",
   "metadata": {},
   "source": [
    "## 일반문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1185b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a4e0a",
   "metadata": {},
   "source": [
    "### 1. 네이버 뉴스 헤드라인\n",
    "배운 내용을 바탕으로 네이버 뉴스(https://news.naver.com/)에서 헤드라인 뉴스의 제목을 추출해보고자 합니다.\n",
    "\n",
    "Q: 다음의 코드에 css selector를 추가하여 최신 기사의 헤드라인을 스크레이핑하는 코드를 완성하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baa56507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                        정은경 “확진자 더 늘 수 있어…2주간 사적모임 취소해달라”\n",
      "                                    \n",
      "\n",
      "                                        [단독] 檢, '생태탕집 모자' 소환...오세훈 내곡동 사건 곧 결론\n",
      "                                    \n",
      "\n",
      "                                        신규확진 3273명…전국적 대확산 초비상\n",
      "                                    \n",
      "\n",
      "                                        코인 거래소 '빅4' 재편...미신고 거래소 인출 서둘러야\n",
      "                                    \n",
      "\n",
      "                                        호남권 개표 앞두고... 이재명 \"본선경쟁력\" 이낙연 \"호남대통령\" 강조\n",
      "                                    \n"
     ]
    }
   ],
   "source": [
    "url = \"https://news.naver.com/\"\n",
    "\n",
    "res = request.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# #today_main_news 바로 아래에 있는\n",
    "# ul 태그 바로 아래에 있는\n",
    "# li 태그 바로 아래에 있는\n",
    "# a 태그 모두 선택\n",
    "selector = \"#today_main_news > div.hdline_news > ul > li > div.hdline_article_tit > a\"  # # css.selector 이용하여 추출\n",
    "\n",
    "for a in soup.select(selector):\n",
    "    title = a.text\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5553a403",
   "metadata": {},
   "source": [
    "### 2. 시민의 소리 게시판\n",
    "다음은 서울시 대공원의 시민의 소리 게시판 입니다.\n",
    "\n",
    "https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgList.do?pgno=1\n",
    "\n",
    "해당 페이지에 나타난 게시글들의 제목을 수집하고자 합니다.\n",
    "\n",
    "Q: 다음의 코드에 css selector를 추가하여 해당 페이지에서 게시글의 제목을 스크레이핑하는 코드를 완성하시오. 또한 과제 제출시 하단의 추가 내용을 참고하여 수집한 데이터를 csv 형태로 저장하여 해당 csv 파일도 함께 제출하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a921b4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['어린이대공원 매점 냉장고 점검 부탁드립니다.', '어린이를 위한 공원내 식당에 아기를 위한 시설 부족(아기의자가 왜 없죠?)', '강창수 해설사님 ', '동물해설사님 칭찬', '강창수 동물 해설사님', '놀이동산 푸드코트 김치가 중국산인 이유는?', '주슨트 설명 최고예요!!', '강창수 주슨트님 최고 !!', 'ZOOCENT 스케줄표?', '호주동물 호주설명 '] ['https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=iH1wOevijC4sqsuSLwqNzzLgwdA7HYzne2xmGvkrzRtrsDEwpR4BzbKWl9mlPdCG.etisw2_servlet_user?qnaid=QNAS20210925000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=iH1wOevijC4sqsuSLwqNzzLgwdA7HYzne2xmGvkrzRtrsDEwpR4BzbKWl9mlPdCG.etisw2_servlet_user?qnaid=QNAS20210923000005&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=iH1wOevijC4sqsuSLwqNzzLgwdA7HYzne2xmGvkrzRtrsDEwpR4BzbKWl9mlPdCG.etisw2_servlet_user?qnaid=QNAS20210920000001&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=iH1wOevijC4sqsuSLwqNzzLgwdA7HYzne2xmGvkrzRtrsDEwpR4BzbKWl9mlPdCG.etisw2_servlet_user?qnaid=QNAS20210919000004&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=iH1wOevijC4sqsuSLwqNzzLgwdA7HYzne2xmGvkrzRtrsDEwpR4BzbKWl9mlPdCG.etisw2_servlet_user?qnaid=QNAS20210919000003&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=iH1wOevijC4sqsuSLwqNzzLgwdA7HYzne2xmGvkrzRtrsDEwpR4BzbKWl9mlPdCG.etisw2_servlet_user?qnaid=QNAS20210918000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=iH1wOevijC4sqsuSLwqNzzLgwdA7HYzne2xmGvkrzRtrsDEwpR4BzbKWl9mlPdCG.etisw2_servlet_user?qnaid=QNAS20210909000001&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=iH1wOevijC4sqsuSLwqNzzLgwdA7HYzne2xmGvkrzRtrsDEwpR4BzbKWl9mlPdCG.etisw2_servlet_user?qnaid=QNAS20210908000004&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=iH1wOevijC4sqsuSLwqNzzLgwdA7HYzne2xmGvkrzRtrsDEwpR4BzbKWl9mlPdCG.etisw2_servlet_user?qnaid=QNAS20210906000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=iH1wOevijC4sqsuSLwqNzzLgwdA7HYzne2xmGvkrzRtrsDEwpR4BzbKWl9mlPdCG.etisw2_servlet_user?qnaid=QNAS20210904000006&pgno=1']\n"
     ]
    }
   ],
   "source": [
    "url_head = \"https://www.sisul.or.kr\"\n",
    "url_board = url_head + \"/open_content/childrenpark/qna/qnaMsgList.do?pgno=1\"\n",
    "\n",
    "res = request.urlopen(url_board)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "selector = \"#detail_con > div.generalboard > table > tbody > tr > td.left.title > a\"\n",
    "titles = []  # 타이틀 리스트 생성\n",
    "links = []  # 링크 리스트 생성\n",
    "for a in soup.select(selector):  # css.selector 이용하여 추출\n",
    "    titles.append(a.text)  # 타이틀 리스트에 텍스트 추가\n",
    "    links.append(url_head + a.attrs[\"href\"])  # 링크 리스트에 a태그의 href값 추가\n",
    "    \n",
    "print(titles, links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca8550d",
   "metadata": {},
   "source": [
    "### 추가 내용\n",
    "수집된 자료를 데이터프레임으로 만들어 csv로 저장하는 것이 일반적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c30bb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>어린이대공원 매점 냉장고 점검 부탁드립니다.</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어린이를 위한 공원내 식당에 아기를 위한 시설 부족(아기의자가 왜 없죠?)</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>강창수 해설사님</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>동물해설사님 칭찬</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>강창수 동물 해설사님</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title  \\\n",
       "0                   어린이대공원 매점 냉장고 점검 부탁드립니다.   \n",
       "1  어린이를 위한 공원내 식당에 아기를 위한 시설 부족(아기의자가 왜 없죠?)   \n",
       "2                                  강창수 해설사님    \n",
       "3                                  동물해설사님 칭찬   \n",
       "4                                강창수 동물 해설사님   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.sisul.or.kr/open_content/childrenp...  \n",
       "1  https://www.sisul.or.kr/open_content/childrenp...  \n",
       "2  https://www.sisul.or.kr/open_content/childrenp...  \n",
       "3  https://www.sisul.or.kr/open_content/childrenp...  \n",
       "4  https://www.sisul.or.kr/open_content/childrenp...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "board_df = pd.DataFrame({\"title\": titles, \"link\": links})  # 데이터프레임 생성\n",
    "board_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07efdebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_df.to_csv(\"board.csv\", index=False)  # csv로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c28666",
   "metadata": {},
   "source": [
    "# (Optional) 웹 크롤링2 - Dynamic Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63a72c",
   "metadata": {},
   "source": [
    "## 0. 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3730094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\user\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "956ebb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804dbb40",
   "metadata": {},
   "source": [
    "## 1. Selenium 기초\n",
    "자신의 크롬 버전을 확인하고 크롬 웹드라이버를 다운받아놓아야합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd2db4",
   "metadata": {},
   "source": [
    "### 1.1. Simple Text Crawling\n",
    "멜론 사이트에서 노래 제목을 크롤링해보자\n",
    "\n",
    "URL: https://www.melon.com/chart/index.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed82d419",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH = 'C:/Users/USER/chromedriver.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9593cca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STAY\\nThe Kid LAROI, Justin Bieber'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # chrome driver 설정\n",
    "driver = webdriver.Chrome(DRIVER_PATH) \n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# title crawling\n",
    "title = WebDriverWait(driver, 20) \\\n",
    "    .until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#frm > div > table > tbody > tr:nth-child(1) > td:nth-child(6) > div > div\")))\n",
    "\n",
    "# print(\"Title: {}\".format(title.text))\n",
    "\n",
    "title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0cff925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My Universe\\nColdplay, 방탄소년단'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2번째 제목 크롤링\n",
    "WebDriverWait(driver, 20) \\\n",
    "    .until(EC.presence_of_element_located((By.XPATH, \"//*[@id='frm']/div/table/tbody/tr[2]/td[6]/div/div\"))).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50835530",
   "metadata": {},
   "source": [
    "### 1.2. Text Crawling with for loop\n",
    "위에서 찾은 Xpath의 규칙을 바탕으로 for loop 만들자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee28d364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STAY\\nThe Kid LAROI, Justin Bieber', 'My Universe\\nColdplay, 방탄소년단', '신호등\\n이무진', 'Permission to Dance\\n방탄소년단', 'OHAYO MY NIGHT\\n디핵 (D-Hack), PATEKO (파테코)', 'Next Level\\naespa', 'Butter\\n방탄소년단', '바라만 본다\\nMSG워너비(M.O.M)', '낙하 (with 아이유)\\nAKMU (악뮤)', 'Weekend\\n태연 (TAEYEON)', 'Dynamite\\n방탄소년단', 'Queendom\\nRed Velvet (레드벨벳)', '이제 나만 믿어요\\n임영웅', '좋아좋아\\n조정석', '시간을 거슬러 (낮에 뜨는 달 X 케이윌)\\n케이윌', 'DUMB DUMB\\n전소미', 'Peaches (Feat. Daniel Caesar & Giveon)\\nJustin Bieber', '다정히 내 이름을 부르면\\n경서예지, 전건호', 'Bad Habits\\nEd Sheeran', 'Sticker\\nNCT 127', '가을 타나 봐\\n이무진', '헤픈 우연\\n헤이즈 (Heize)', '별빛 같은 나의 사랑아\\n임영웅', '그대라는 사치\\n임영웅', 'HERO\\n임영웅', '비와 당신\\n이무진', '다시 사랑한다면 (김필 Ver.)\\n임영웅', 'Lemonade\\nNCT 127', 'Savage Love (Laxed - Siren Beat) (BTS Remix)\\nJawsh 685, Jason Derulo, 방탄소년단', '고백\\n멜로망스', '작은 것들을 위한 시 (Boy With Luv) (Feat. Halsey)\\n방탄소년단', '끝사랑\\n임영웅', 'Dun Dun Dance\\n오마이걸 (OH MY GIRL)', 'Bk Love\\n임영웅', \"롤린 (Rollin')\\n브레이브걸스\", '잊었니\\n임영웅', '라일락\\n아이유', 'ASAP\\nSTAYC(스테이씨)', '색안경 (STEREOTYPE)\\nSTAYC(스테이씨)', '봄날\\n방탄소년단', 'Celebrity\\n아이유', '사이렌 Remix (Feat. UNEDUCATED KID, Paul Blanco)\\n호미들', '찰나가 영원이 될 때 (The Eternal Moment)\\n마크툽 (MAKTUB)', 'Life Goes On\\n방탄소년단', '비가 오는 날엔 (2021)\\n헤이즈 (Heize)', 'Road Trip\\nNCT 127', '가을 우체국 앞에서\\n김대명', '미워요\\n임영웅', '사랑의 아픔 딛고\\n임영웅', '계단말고 엘리베이터\\n임영웅']\n"
     ]
    }
   ],
   "source": [
    "# chrome driver 설정\n",
    "driver = webdriver.Chrome(DRIVER_PATH)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 빈 리스트 변수\n",
    "title_list = []\n",
    "\n",
    "# title crawling (TOP 50)\n",
    "for i in range(1, 51):\n",
    "    title = WebDriverWait(driver, 20) \\\n",
    "        .until(EC.presence_of_element_located((By.XPATH, f\"//*[@id='frm']/div/table/tbody/tr[{i}]/td[6]/div/div\")))\n",
    "    title_list.append(title.text)\n",
    "    \n",
    "print(title_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8419fbb7",
   "metadata": {},
   "source": [
    "### 1.3. Text Crawling (Click & Back)\n",
    "클릭하고 나오기 -> 동적 크롤링 가능 (가사 크롤링 가능)\n",
    "\n",
    "노래 제목에 링크가 걸려있기 때문에, 해당 링크까지의 XPath를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "782ad777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chrome driver 설정\n",
    "driver = webdriver.Chrome(DRIVER_PATH)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 1번째 click하기\n",
    "click_element = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"frm\"]/div/table/tbody/tr[1]/td[5]/div/a')))\n",
    "click_element.click()    \n",
    "\n",
    "# back\n",
    "driver.back()\n",
    "\n",
    "\n",
    "# 2번째 click하기\n",
    "click_element = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"frm\"]/div/table/tbody/tr[2]/td[5]/div/a')))\n",
    "click_element.click()    \n",
    "\n",
    "# back\n",
    "driver.back()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e88ea",
   "metadata": {},
   "source": [
    "### 1.4. Text Crawling including contents\n",
    "* 1.2처럼 for문과 함께 써보자! (첫 페이지 5개의 글에 대해 title, artist, heart(하트 갯수), lyrics(가사)를 크롤링\n",
    "\n",
    "* 1.3에서 사용한 click & back을 활용하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b0e765a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STAY', 'My Universe', '신호등', 'Permission to Dance', 'OHAYO MY NIGHT']\n",
      "['The Kid LAROI', 'Coldplay', '이무진', '방탄소년단', '디핵 (D-Hack)']\n",
      "['151,399', '63,179', '214,621', '175,985', '132,719']\n",
      "[\"I do the same thing I told you\\nthat I never would\\nI told you I'd change\\neven when I knew I never could\\nI know that I can't find\\nnobody else\\nas good as you\\nI need you to stay\\nneed you to stay hey Oh\\nI get drunk wake up\\nI'm wasted still\\nI realize the time\\nthat I wasted here\\nI feel like you can't\\nfeel the way I feel\\nOh I'll be fucked up\\nif you can't be right here\\nOh ooh-woah\\nOh ooh-woah ooh-woah\\nOh ooh-woah\\nOh ooh-woah ooh-woah\\nOh ooh-woah\\nOh ooh-woah ooh-woah\\nOh I'll be fucked up\\nif you can't be right here\\nI do the same thing I told you\\nthat I never would\\nI told you I'd change\\neven when I knew I never could\\nI know that I can't find\\nnobody else\\nas good as you\\nI need you to stay\\nneed you to stay hey\\nI do the same thing I told you\\nthat I never would\\nI told you I'd change\\neven when I knew I never could\\nI know that I can't find\\nnobody else\\nas good as you\\nI need you to stay\\nneed you to stay hey\\nWhen I'm away from you\\nI miss your touch Ooh\\nYou're the reason\\nI believe in love Ooh\\nIt's been difficult\\nfor me to trust Ooh\\nAnd I'm afraid\\nthat I'ma fuck it up Ooh\\nAin't no way\\nthat I can leave you stranded\\n'Cause you ain't ever left me\\nempty-handed\\nAnd you know that I know\\nthat I can't live without you\\nSo baby stay\\nOh ooh-woah\\nOh ooh-woah ooh-woah\\nOh ooh-woah\\nOh ooh-woah ooh-woah\\nOh ooh-woah\\nOh ooh-woah ooh-woah\\nI'll be fucked up\\nif you can't be right here\\nI do the same thing I told you\\nthat I never would\\nI told you I'd change\\neven when I knew I never could\\nI know that I can't find\\nnobody else\\nas good as you\\nI need you to stay\\nneed you to stay hey\\nI do the same thing\\nI told you that I never would\\nI told you I'd change\\neven when I knew I never could\\nI know that I can't find\\nnobody else\\nas good as you\\nI need you to stay\\nneed you to stay hey\\nWoah-oh\\nI need you to stay\\nneed you to stay hey\", 'You, you are my universe and\\nI just want to put you first\\nAnd you, you are my universe, and I…\\n\\nIn the night I lie and look up at you\\nWhen the morning comes I watch you rise\\nThere’s a paradise they couldn’t capture\\nThat bright infinity inside your eyes\\n\\n매일 밤 네게 날아가 (가)\\n꿈이란 것도 잊은 채\\n나 웃으며 너를 만나 (나)\\nNever ending forever baby\\n\\nYou, you are my universe and\\nI just want to put you first\\nAnd you, you are my universe, and\\nYou make my world light up inside\\n\\n어둠이 내겐 더 편했었지\\n길어진 그림자 속에서 (eyes)\\n\\nAnd they said that we can’t be together\\nBecause\\nBecause we come from different sides\\n\\nYou, you are my universe and\\nI just want to put you first\\nAnd you, you are my universe, and\\nYou make my world light up inside\\n\\nMy universe (do do, do do)\\nMy universe (do do, do do)\\nMy universe (do do, do do)\\n(you make my world)\\nYou make my world light up inside\\n\\nMake my world light up inside\\n\\n나를 밝혀주는 건\\n너란 사랑으로 수 놓아진 별\\n내 우주의 넌\\n또 다른 세상을 만들어 주는 걸\\n\\n너는 내 별이자 나의 우주니까\\n지금 이 시련도 결국엔 잠시니까\\n너는 언제까지나 지금처럼 밝게만 빛나줘\\n우리는 너를 따라 이 긴 밤을 수놓을 거야\\n\\n너와 함께 날아가 (가)\\nWhen I’m without you I’m crazy\\n자 어서 내 손을 잡아 (아)\\nWe are made of each other baby\\n\\nYou, you are my universe and\\nI just want to put you first\\nAnd you, you are my universe, and\\nYou make my world light up inside\\n\\nMy universe (you, you are)\\nMy universe (I just want)\\nMy universe (you, you are)\\nMy universe, and I\\n\\nMy universe', '이제야 목적지를 정했지만\\n가려한 날 막아서네 난 갈 길이 먼데\\n새빨간 얼굴로 화를 냈던\\n친구가 생각나네\\n이미 난 발걸음을 떼었지만\\n가려한 날 재촉하네 걷기도 힘든데\\n새파랗게 겁에 질려 도망간\\n친구가 뇌에 맴도네\\n건반처럼 생긴 도로 위\\n수많은 동그라미들 모두가\\n멈췄다 굴렀다 말은 잘 들어\\n그건 나도 문제가 아냐\\n붉은색 푸른색 그 사이\\n3초 그 짧은 시간\\n노란색 빛을 내는\\n저기 저 신호등이\\n내 머릿속을 텅 비워버려\\n내가 빠른지도\\n느린지도 모르겠어\\n그저 눈앞이 샛노랄 뿐이야\\n솔직히 말하자면 차라리\\n운전대를 못 잡던 어릴 때가\\n더 좋았었던 것 같아\\n그땐 함께 온 세상을\\n거닐 친구가 있었으니\\n건반처럼 생긴 도로 위\\n수많은 조명들이 날 빠르게\\n번갈아 가며 비추고 있지만\\n난 아직 초짜란 말이야\\n붉은색 푸른색 그 사이\\n3초 그 짧은 시간\\n노란색 빛을 내는 저기 저 신호등이\\n내 머릿속을 텅 비워버려\\n내가 빠른지도\\n느린지도 모르겠어\\n그저 눈앞이 샛노랄 뿐이야\\n꼬질꼬질한 사람이나\\n부자 곁엔 아무도 없는\\n삼색 조명과 이색 칠 위에\\n서 있어 괴롭히지 마\\n붉은색 푸른색 그 사이\\n3초 그 짧은 시간\\n노란색 빛을 내는 저기 저 신호등이\\n내 머릿속을 텅 비워버려\\n내가 빠른지도\\n느린지도 모르겠어\\n그저 눈앞이 샛노랄 뿐이야', 'It’s the thought of being young\\nWhen your heart’s just like a drum\\nBeating louder with no way to guard it\\nWhen it all seems like it’s wrong\\nJust sing along to Elton John\\nAnd to that feeling, we’re just getting started\\n\\nWhen the nights get colder\\nAnd the rhythms got you falling behind\\nJust dream about that moment\\nWhen you look yourself right in the eye, eye, eye\\nThen you say\\n\\nI wanna dance\\nThe music’s got me going\\nAin’t nothing that can stop how we move yeah\\nLet’s break our plans\\nAnd live just like we’re golden\\nAnd roll in like we’re dancing fools\\n\\nWe don’t need to worry\\n‘Cause when we fall we know how to land\\nDon’t need to talk the talk, just walk the walk tonight\\n‘Cause we don’t need permission to dance\\n\\nThere’s always something that’s standing in the way\\nBut if you don’t let it faze ya\\nYou’ll know just how to break\\nJust keep the right vibe yeah\\n‘Cause there’s no looking back\\nThere ain’t no one to prove\\nWe don’t got this on lock yeah\\n\\nThe wait is over\\nThe time is now so let’s do it right\\nYeah we’ll keep going\\nAnd stay up until we see the sunrise\\nAnd we’ll say\\n\\nI wanna dance\\nThe music’s got me going\\nAin’t nothing that can stop how we move yeah\\nLet’s break our plans\\nAnd live just like we’re golden\\nAnd roll in like we’re dancing fools\\n\\nWe don’t need to worry\\n‘Cause when we fall we know how to land\\nDon’t need to talk the talk, just walk the walk tonight\\n‘Cause we don’t need permission to dance\\n\\nDa na na na na na na\\nDa na na na na na na\\nDa na na na na na na\\nNo, we don’t need permission to dance\\n\\nDa na na na na na na\\nDa na na na na na na\\nDa na na na na na na\\n\\nWell let me show ya\\nThat we can keep the fire alive\\n‘Cause it’s not over\\nTill it’s over say it one more time\\nSay\\n\\nI wanna dance\\nThe music’s got me going\\nAin’t nothing that can stop how we move yeah\\nLet’s break our plans\\nAnd live just like we’re golden\\nAnd roll in like we’re dancing fools\\n\\nWe don’t need to worry\\n‘Cause when we fall we know how to land\\nDon’t need to talk the talk, just walk the walk tonight\\n‘Cause we don’t need permission to dance', '너를 사랑하고 있어\\n너를 사랑하고 있어\\n자기야 날 사랑해주면 안 될까\\n말처럼 쉽지는 않은 걸 알지만\\n세게 날 안아주면 안 될까\\n오늘따라 세상이 무섭단 말이야\\n잠깐 인공호흡을 해주라\\n왠지 숨이 잘 안 쉬어져서 난\\n날 놓을 거면 과거에 놔주라\\n네가 있는 시간에서 죽어갈 거야\\n우리 그냥 결혼하면 안 될까\\n돈은 내가 열심히 벌 테니까\\n이 세상과 내가 눈감는 날\\n까지만 날 사랑한다 말해주라\\n내가 너를 사랑해도\\n네가 날 안 사랑해도\\n우린 나름대로 행복할 거야\\n내 방 천장에 그려 본\\n내 우주에게 물어본\\n말은 나를 사랑하면 안 될까\\n오사카나 오키나와의 바다\\n내 뮤비들을 찍었던 곳 말이야\\n같이 가자 약속했었잖아\\n그 약속이 깨질까 봐 겁이 나\\nWHUTUF이 결혼한다 하던 날\\n진짜 처음으로 걔가 부럽더라\\n하얀 웨딩드레스를 입은 아름다운\\n너와 영원을 말할 수 있을까\\n가족이 되어주라\\n내 집이 되어주라\\n나도 날 줄 테니 너도 널 주라\\n평생의 연인이야\\n네 말대로 말이야\\n그래 별과 우주잖아\\n날 사랑하지 않는다면\\n나의 사랑 반을 받아\\n남은 사랑의 반도\\n내가 채워줄 거야 꼭\\n내가 너를 사랑해도\\n네가 날 안 사랑해도\\n우린 나름대로 행복할 거야\\n내 방 천장에 그려 본\\n내 우주에게 물어본\\n말은 나를 사랑하면 안 될까\\n내가 너를 사랑해도\\n네가 날 안 사랑해도\\n우린 나름대로 행복할 거야\\n내 방 천장에 그려 본\\n내 우주에게 물어본\\n말은 나를 사랑하면 안 될까']\n"
     ]
    }
   ],
   "source": [
    "# chrome driver 설정\n",
    "driver = webdriver.Chrome(DRIVER_PATH)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 빈 리스트 변수\n",
    "title_list = []\n",
    "artist_list = []\n",
    "heart_list = []\n",
    "lyrics_list = []\n",
    "\n",
    "# crawling (TOP 5)\n",
    "for i in range(1, 6):\n",
    "    # click\n",
    "    click_element = WebDriverWait(driver, 20) \\\n",
    "        .until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"frm\"]/div/table/tbody/tr[{i}]/td[5]/div/a')))\n",
    "    click_element.click()\n",
    "\n",
    "    # title crawling\n",
    "    title = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"#downloadfrm > div > div > div.entry > div.info > div.song_name\")))\n",
    "    title_list.append(title.text)\n",
    "\n",
    "    # artist crawling\n",
    "    artist = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"#downloadfrm > div > div > div.entry > div.info > div.artist > a > span:nth-child(1)\")))\n",
    "    artist_list.append(artist.text)\n",
    "    \n",
    "    # heart crawling\n",
    "    heart = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"#d_like_count\")))\n",
    "    heart_list.append(heart.text)\n",
    "\n",
    "    # lyrics crawling\n",
    "    lyrics = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"#d_video_summary\")))\n",
    "    lyrics_list.append(lyrics.text)\n",
    "    \n",
    "    # back\n",
    "    driver.back()\n",
    "    \n",
    "print(title_list)\n",
    "print(artist_list)\n",
    "print(heart_list)\n",
    "print(lyrics_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33837613",
   "metadata": {},
   "source": [
    "### TIP: 보통은 결과값을 데이터프레임 형태로 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f932cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 변수\n",
    "raw_result = {'title': title_list,\n",
    "              'artist': artist_list,\n",
    "              'heart': heart_list,\n",
    "          'lyrics': lyrics_list}\n",
    "\n",
    "result = pd.DataFrame(raw_result)\n",
    "\n",
    "# # csv 파일로 save\n",
    "# result.to_csv(\"MelonTop5\", mode='w')\n",
    "\n",
    "# driver 종료\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e3b005d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>heart</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STAY</td>\n",
       "      <td>The Kid LAROI</td>\n",
       "      <td>151,399</td>\n",
       "      <td>I do the same thing I told you\\nthat I never w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My Universe</td>\n",
       "      <td>Coldplay</td>\n",
       "      <td>63,179</td>\n",
       "      <td>You, you are my universe and\\nI just want to p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>신호등</td>\n",
       "      <td>이무진</td>\n",
       "      <td>214,621</td>\n",
       "      <td>이제야 목적지를 정했지만\\n가려한 날 막아서네 난 갈 길이 먼데\\n새빨간 얼굴로 화...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Permission to Dance</td>\n",
       "      <td>방탄소년단</td>\n",
       "      <td>175,985</td>\n",
       "      <td>It’s the thought of being young\\nWhen your hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OHAYO MY NIGHT</td>\n",
       "      <td>디핵 (D-Hack)</td>\n",
       "      <td>132,719</td>\n",
       "      <td>너를 사랑하고 있어\\n너를 사랑하고 있어\\n자기야 날 사랑해주면 안 될까\\n말처럼 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 title         artist    heart  \\\n",
       "0                 STAY  The Kid LAROI  151,399   \n",
       "1          My Universe       Coldplay   63,179   \n",
       "2                  신호등            이무진  214,621   \n",
       "3  Permission to Dance          방탄소년단  175,985   \n",
       "4       OHAYO MY NIGHT    디핵 (D-Hack)  132,719   \n",
       "\n",
       "                                              lyrics  \n",
       "0  I do the same thing I told you\\nthat I never w...  \n",
       "1  You, you are my universe and\\nI just want to p...  \n",
       "2  이제야 목적지를 정했지만\\n가려한 날 막아서네 난 갈 길이 먼데\\n새빨간 얼굴로 화...  \n",
       "3  It’s the thought of being young\\nWhen your hea...  \n",
       "4  너를 사랑하고 있어\\n너를 사랑하고 있어\\n자기야 날 사랑해주면 안 될까\\n말처럼 ...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b34f86",
   "metadata": {},
   "source": [
    "## 2. Image Crawling\n",
    "이미지 크롤링하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67600bf3",
   "metadata": {},
   "source": [
    "#### STEP1. URL Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7e66e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://cdnimg.melon.co.kr/cm2/album/images/106/46/395/10646395_20210707141710_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/107/20/913/10720913_20210923173742_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/07/796/10607796_20210513201807_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/48/182/10648182_20210709104950_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/104/47/520/10447520_20200619123343_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/09/232/10609232_20210517155130_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/12/483/10612483_20210521111412_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/38/275/10638275_20210625172521_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/61/658/10661658_20210726111159_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/45/654/10645654_20210706155154_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/104/79/150/10479150_20200918102847_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/80/450/10680450_20210813124748_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/104/12/319/10412319_20200403103006_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/53/694/10653694_20210715164901_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/98/116/10698116_20210831104635_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/67/450/10667450_20210802111127_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/105/80/103/10580103_20210319132819_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/10/525/10610525_20210518143433_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/37/411/10637411_20210909170255_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/107/03/942/10703942_20210917110116_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/107/12/767/10712767_20210913165623_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/11/845/10611845_20210520170350_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/105/75/005/10575005_20210309113840_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/44/845/10644845_20210705203115_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/105/13/079/10513079_20201103201136_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/31/122/10631122_20210617142653_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/14/238/10614238_20210525100205_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/107/03/942/10703942_20210917110116_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/104/98/123/10498123_20201002094556_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/95/590/10695590_20210827162225_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm/album/images/102/73/641/10273641_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/39/384/10639384_20210628195604_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/04/729/10604729_20210510143932_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/32/758/10632758_20210621102906_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/100/43/575/10043575_20210302112520_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/33/915/10633915_20210622101307_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/105/54/246/10554246_20210325161233_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/105/89/127/10589127_20210407175809_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/107/04/178/10704178_20210906141809_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm/album/images/100/37/969/10037969_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/105/54/246/10554246_20210325161233_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/105/80/227/10580227_20210319163608_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/107/16/174/10716174_20210916153439_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/105/21/521/10521521_20201120112220_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/70/618/10670618_20210804111639_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/107/03/942/10703942_20210917110116_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/36/269/10636269_20210625102856_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm/album/images/027/02/825/2702825_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm2/album/images/106/62/269/10662269_20210726181721_500.jpg/melon/resize/120/quality/80/optimize', 'https://cdnimg.melon.co.kr/cm/album/images/101/95/495/10195495_500.jpg/melon/resize/120/quality/80/optimize']\n"
     ]
    }
   ],
   "source": [
    "# chrome driver 설정\n",
    "driver = webdriver.Chrome(DRIVER_PATH)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 빈 리스트 변수\n",
    "link_list = []\n",
    "\n",
    "# # img crawling (TOP 50)\n",
    "for i in range(1, 51):\n",
    "    \n",
    "    img = WebDriverWait(driver, 20) \\\n",
    "        .until(EC.presence_of_element_located((By.CSS_SELECTOR, f\"#frm > div > table > tbody > tr:nth-child({i}) > td:nth-child(4) > div > a > img\")))\n",
    "\n",
    "    link_list.append(img.get_attribute('src'))\n",
    "\n",
    "print(link_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3206cdef",
   "metadata": {},
   "source": [
    "#### STEP2. Download images using URLs\n",
    "자신의 디렉토리에 img 폴더 생성하고 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b25cf922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "count = 0\n",
    "for link in link_list:\n",
    "    count += 1\n",
    "    urllib.request.urlretrieve(link, 'C:/Users/USER/img' + str(count) + '.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
